{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmRC+xfFlCsTad0g1vfQmC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dy99-ux/Polytex-EDM/blob/main/Enterprise_Datamodel\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPofDf8TfhNW",
        "outputId": "4d520df1-e884-4260-89eb-f26ad58f6d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EDM export complete.\n"
          ]
        }
      ],
      "source": [
        "# edited column names to polytex_site1_area3_line2_sensor_temperature\n",
        "# this file excludes johns code as it puts out error\n",
        "# Sensor functionality data for line 2 - Doyeon\n",
        "# edited column names to polytex_site1_area3_line2_sensor_temperature\n",
        "# this file excludes johns code as it puts out error\n",
        "# Sensor functionality data for line 2 - Doyeon\n",
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/dy99-ux/sensor-data/refs/heads/main/sensor_status.csv\"\n",
        "\n",
        "response = requests.get(url)          # API-style HTTP GET request\n",
        "csv_data = response.text              # raw CSV text returned by the server\n",
        "\n",
        "df = pd.read_csv(StringIO(csv_data))  # convert CSV text → DataFrame\n",
        "# df.head() # Commenting out display for script\n",
        "\n",
        "class Asset:\n",
        "    def __init__(self, name, sensor_prefix):\n",
        "        self.name = name\n",
        "        self.sensor_prefix = sensor_prefix\n",
        "        self.data = None\n",
        "\n",
        "    def load_data(self, master_df):\n",
        "        columns_needed = [\n",
        "            \"date\",\n",
        "            f\"{self.sensor_prefix}_temperature\",\n",
        "            f\"{self.sensor_prefix}_flow\",\n",
        "            f\"{self.sensor_prefix}_pressure\"\n",
        "        ]\n",
        "        self.data = master_df[columns_needed].copy() # Added .copy() here to avoid SettingWithCopyWarning\n",
        "\n",
        "    def get_sensor_status(self):\n",
        "        return self.data\n",
        "\n",
        "Line1 = Asset(\"Line 1\", \"polytex_site1_area3_line1_sensor\")\n",
        "Line2 = Asset(\"Line 2\", \"polytex_site1_area3_line2_sensor\")\n",
        "Line3 = Asset(\"Line 3\", \"polytex_site1_area3_line3_sensor\")\n",
        "Line4 = Asset(\"Line 4\", \"polytex_site1_area3_line4_sensor\")\n",
        "\n",
        "# Load data into each Asset object\n",
        "Line1.load_data(df)\n",
        "Line2.load_data(df)\n",
        "Line3.load_data(df)\n",
        "Line4.load_data(df)\n",
        "\n",
        "# Now use the data attribute of each Asset object\n",
        "Line1.data[\"sensor_available\"] = (\n",
        "    Line1.data[f\"{Line1.sensor_prefix}_temperature\"] &\n",
        "    Line1.data[f\"{Line1.sensor_prefix}_flow\"] &\n",
        "    Line1.data[f\"{Line1.sensor_prefix}_pressure\"]\n",
        ")\n",
        "\n",
        "\n",
        "Line2.data[\"sensor_available\"] = (\n",
        "    Line2.data[f\"{Line2.sensor_prefix}_temperature\"] &\n",
        "    Line2.data[f\"{Line2.sensor_prefix}_flow\"] &\n",
        "    Line2.data[f\"{Line2.sensor_prefix}_pressure\"]\n",
        ")\n",
        "\n",
        "Line3.data[\"sensor_available\"] = (\n",
        "    Line3.data[f\"{Line3.sensor_prefix}_temperature\"] &\n",
        "    Line3.data[f\"{Line3.sensor_prefix}_flow\"] &\n",
        "    Line3.data[f\"{Line3.sensor_prefix}_pressure\"]\n",
        ")\n",
        "\n",
        "Line4.data[\"sensor_available\"] = (\n",
        "    Line4.data[f\"{Line4.sensor_prefix}_temperature\"] &\n",
        "    Line4.data[f\"{Line4.sensor_prefix}_flow\"] &\n",
        "    Line4.data[f\"{Line4.sensor_prefix}_pressure\"]\n",
        ")\n",
        "\n",
        "Line1.data[\"line1_auto_shutdown_triggered\"] = ~Line1.data[\"sensor_available\"]\n",
        "line1_auto_shutdown_events = Line1.data[Line1.data[\"line1_auto_shutdown_triggered\"]]\n",
        "line1_auto_shutdown_events.head()\n",
        "\n",
        "Line2.data[\"line2_auto_shutdown_triggered\"] = ~Line2.data[\"sensor_available\"]\n",
        "line2_auto_shutdown_events = Line2.data[Line2.data[\"line2_auto_shutdown_triggered\"]]\n",
        "line2_auto_shutdown_events.head()\n",
        "\n",
        "Line3.data[\"line3_auto_shutdown_triggered\"] = ~Line3.data[\"sensor_available\"]\n",
        "line3_auto_shutdown_events = Line3.data[Line3.data[\"line3_auto_shutdown_triggered\"]]\n",
        "line3_auto_shutdown_events.head()\n",
        "\n",
        "Line4.data[\"line4_auto_shutdown_triggered\"] = ~Line4.data[\"sensor_available\"]\n",
        "line4_auto_shutdown_events = Line4.data[Line4.data[\"line4_auto_shutdown_triggered\"]]\n",
        "line4_auto_shutdown_events.head()\n",
        "\n",
        "# Export each line's shutdown events as a dataset\n",
        "Line1.data.to_csv(\"polytex_site1_area3_line1_sensor_data.csv\", index=False)\n",
        "Line2.data.to_csv(\"polytex_site1_area3_line2_sensor_data.csv\", index=False)\n",
        "Line3.data.to_csv(\"polytex_site1_area3_line3_sensor_data.csv\", index=False)\n",
        "Line4.data.to_csv(\"polytex_site1_area3_line4_sensor_data.csv\", index=False)\n",
        "\n",
        "print(\"EDM export complete.\")\n",
        "\n",
        "\n",
        "# This dataset is federated since the sensor availability for each lines are stored in each lines.\n",
        "# It can be integrated into anomaly detection on level 2 of our S95 model.\n",
        "# This is a closed loop use case since the operation is automatically paused and recorded when the sensor does not work and maintenance is alerted real-time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# First pass yield data for area 1 - Doyeon\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# --- Location ---\n",
        "latitude, longitude = 43.6629, -79.3957\n",
        "timezone = \"America/Toronto\"\n",
        "\n",
        "# --- Date range: past year ---\n",
        "end_date = date.today()\n",
        "start_date = end_date - timedelta(days=365)\n",
        "\n",
        "# --- Get daily max temperature ---\n",
        "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "params = {\n",
        "    \"latitude\": latitude,\n",
        "    \"longitude\": longitude,\n",
        "    \"start_date\": start_date.isoformat(),\n",
        "    \"end_date\": end_date.isoformat(),\n",
        "    \"daily\": [\"temperature_2m_max\"],\n",
        "    \"timezone\": timezone\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# --- Convert to DataFrame ---\n",
        "daily_data = pd.DataFrame({\n",
        "    \"date\": data[\"daily\"][\"time\"],\n",
        "    \"first pass yield\": data[\"daily\"][\"temperature_2m_max\"]\n",
        "})\n",
        "\n",
        "\n",
        "# --- Create yield values ---\n",
        "daily_data[\"area1_first_pass_yield\"] = (\n",
        "    daily_data[\"first pass yield\"] + 70\n",
        ")\n",
        "\n",
        "\n",
        "# --- Save to CSV file ---\n",
        "output_path = \"daily_quality_control.csv\"\n",
        "daily_data[[\"date\", \"area1_first_pass_yield\"]].to_csv(output_path, index=False)\n",
        "\n",
        "# print(f\"CSV file saved as: {output_path}\") # Commenting out display for script\n",
        "\n",
        "# pd.read_csv(\"daily_quality_control.csv\") # Commenting out display for script\n",
        "\n",
        "\n",
        "# This data set is monolithic as the yield for each area is stored in the main cloud for the enterprise.\n",
        "# This can be used in manufacturing operation management, which is level 3 on our S95 model.\n",
        "# This is an open loop metric since it is a result before any correction is done and no action is done automatically.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DwSbYgqYgErf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Petrochemical Plant Process Data - Ardin\n",
        "# Scraping XML/HTML Data using beautifulsoup\n",
        "\n",
        "# Imports\n",
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Scraping\n",
        "petr_url = \"https://books.toscrape.com/\"\n",
        "\n",
        "petr_response = requests.get(petr_url)\n",
        "soup = BeautifulSoup(petr_response.text, \"html.parser\")\n",
        "\n",
        "rating_map = {\n",
        "    \"One\": 1,\n",
        "    \"Two\": 2,\n",
        "    \"Three\": 3,\n",
        "    \"Four\": 4,\n",
        "    \"Five\": 5\n",
        "}\n",
        "\n",
        "def title_hash(title):\n",
        "    return abs(hash(title)) % 6 + 10   # maps into 90–100\n",
        "\n",
        "books_data = []\n",
        "\n",
        "for book in soup.select(\".product_pod\"):\n",
        "    title = book.h3.a[\"title\"]\n",
        "\n",
        "    # Remove currency symbol, convert to float\n",
        "    price_text = book.select_one(\".price_color\").text.strip()\n",
        "    price_float = float(price_text.replace(\"Â£\", \"\"))\n",
        "\n",
        "    # Change rating strings to float\n",
        "    rating_word = book.select_one(\".star-rating\")[\"class\"][1]\n",
        "    rating_int = rating_map.get(rating_word, None)\n",
        "\n",
        "    books_data.append({\n",
        "        \"Reactor_Temperature (C)\": ((price_float%10)+10)*10,\n",
        "        \"Reactor_Water_Content (%)\": (rating_int+5)*3+random.random(),\n",
        "        \"Reactor Pressure (kg/cm2g)\": title_hash(title)+random.random()\n",
        "    })\n",
        "\n",
        "# Data Generation using the scrapped data to produce realistic process historian data with 5s sampling rate.\n",
        "petr_df = pd.DataFrame(books_data)\n",
        "\n",
        "# Defining initial values and steps for the random walk\n",
        "initial = petr_df.iloc[random.randint(0,len(petr_df))]\n",
        "steps = 1000\n",
        "time_index = pd.date_range(start=date.today(), periods=steps, freq=\"5S\")\n",
        "\n",
        "noise_scale = {\n",
        "    \"Reactor_Temperature (C)\": 0.05,\n",
        "    \"Reactor_Water_Content (%)\": 0.01,\n",
        "    \"Reactor Pressure (kg/cm2g)\": 0.02,\n",
        "}\n",
        "\n",
        "rw_df = pd.DataFrame([initial] * steps, index=time_index)\n",
        "\n",
        "# Apply cumulative random walk noise\n",
        "for col in rw_df.columns:\n",
        "    noise = np.random.normal(loc=0, scale=noise_scale[col], size=steps).cumsum()\n",
        "    rw_df[col] = rw_df[col] + noise\n",
        "\n",
        "# rw_df.head(10) # Commenting out display for script\n",
        "\n",
        "\n",
        "# Plotting the Process Historian Data\n",
        "# fig, axs = plt.subplots(3, 1, figsize=(15, 12)) # Commenting out plotting for script\n",
        "# axs[0].plot(rw_df[\"Reactor_Temperature (C)\"])\n",
        "# axs[0].set_title(\"Reactor Temperature\")\n",
        "# axs[0].set_ylabel(\"Temperature (C)\")\n",
        "# axs[1].plot(rw_df[\"Reactor_Water_Content (%)\"])\n",
        "# axs[1].set_title(\"Reactor Water Content\")\n",
        "# axs[1].set_ylabel(\"Water Content (%)\")\n",
        "# axs[2].plot(rw_df[\"Reactor Pressure (kg/cm2g)\"])\n",
        "# axs[2].set_title(\"Reactor Pressure\")\n",
        "# axs[2].set_ylabel(\"Pressure (kg/cm2g)\")\n",
        "# plt.tight_layout()\n",
        "# plt.show() # Commenting out plotting for script\n",
        "\n",
        "\n",
        "# Reading Safety HAZOP data from excel file\n",
        "# Safety data helps with more sophisticated way to deal with missing values in a control system\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from openpyxl import load_workbook\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the Excel file\n",
        "safety_url = \"https://raw.githubusercontent.com/ardin-hsr/CHE1149-Assignment-4/main/petrochemical_plant_HAZOP.xlsx\"\n",
        "\n",
        "# Download the file\n",
        "safety_response = requests.get(safety_url)\n",
        "\n",
        "# Load workbook from bytes\n",
        "safety_workbook = load_workbook(filename=BytesIO(safety_response.content))\n",
        "\n",
        "# Select the first sheet\n",
        "sheet = safety_workbook.active\n",
        "\n",
        "# Extract data\n",
        "data = []\n",
        "for row in sheet.iter_rows(values_only=True):\n",
        "    data.append(row)\n",
        "\n",
        "# Convert to DataFrame\n",
        "safety_df = pd.DataFrame(data[1:], columns=data[0])\n",
        "# pd.set_option(\"display.max_colwidth\", None) # Commenting out display option for script\n",
        "# safety_df[safety_df['Equipment']=='Reactor'] # Commenting out display for script\n",
        "\n",
        "\n",
        "# Based on the HAZOP sheet, if we were to use data such as reactor_temperature, reactor_pressure, and reactor_water in a control system and one of those data has a missing value, it would be\n",
        "# safer to bring the ractor to a state of low catalyst concentration rather than high catalyst concentration. Thus, we would deal with missing data in the control system by for example setting\n",
        "# catalyst flow set point to 0 kg/h.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHWgMy1QgBYO",
        "outputId": "c27d3fe8-bf81-4572-b6d7-eb729c6063d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2692884559.py:52: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
            "  time_index = pd.date_range(start=date.today(), periods=steps, freq=\"5S\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Textile Plant_ Recycled water parameters data_Nida\n",
        "# (Fetching USGS Water Parameters Data)\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# site\n",
        "site = \"09380000\"\n",
        "\n",
        "\n",
        "# The keys are USGS parameter codes, values are the desired column names.\n",
        "param_code = {\n",
        "    \"00010\": \"Temperature (°C)\",\n",
        "    \"00060\": \"Level (l)\",\n",
        "    \"00400\": \"pH\"\n",
        "}\n",
        "\n",
        "# Extract the list of parameter codes for the USGS API query from the mapping\n",
        "parameters = list(param_code.keys())\n",
        "\n",
        "# Date range\n",
        "start = \"2024-01-01\"\n",
        "end = \"2024-12-31\"\n",
        "\n",
        "def get_usgs_data(site, params_map):\n",
        "    \"\"\"Fetch multiple parameters for a site from USGS, using a parameter code map\"\"\"\n",
        "    param_str = \",\".join(params_map.keys())\n",
        "    url = (\n",
        "        \"https://waterservices.usgs.gov/nwis/dv/\"\n",
        "        f\"?format=json&sites={site}&parameterCd={param_str}\"\n",
        "        f\"&startDT={start}&endDT={end}&siteStatus=all\"\n",
        "    ) #API requires request to be in this format\n",
        "\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "\n",
        "    collected_series_dfs = {} # Use a dictionary to store one DataFrame per param_code\n",
        "    try:\n",
        "        for ts in data[\"value\"][\"timeSeries\"]:\n",
        "            param_code = ts[\"variable\"][\"variableCode\"][0]['value']\n",
        "\n",
        "            # Only process if we want this parameter and haven't already collected it\n",
        "            if param_code in params_map and param_code not in collected_series_dfs:\n",
        "                param_name = params_map[param_code]\n",
        "\n",
        "                # Extract data values\n",
        "                series_data = ts[\"values\"][0][\"value\"]\n",
        "                df_param = pd.DataFrame(series_data)\n",
        "                df_param[\"dateTime\"] = pd.to_datetime(df_param[\"dateTime\"])\n",
        "                df_param[\"value\"] = pd.to_numeric(df_param[\"value\"], errors=\"coerce\")\n",
        "\n",
        "                # Rename the 'value' column to the desired Parameter name\n",
        "                df_param = df_param[[\"dateTime\", \"value\"]].rename(columns={\"value\": param_name})\n",
        "                collected_series_dfs[param_code] = df_param.set_index(\"dateTime\")\n",
        "    except (KeyError, IndexError):\n",
        "        pass\n",
        "\n",
        "    if collected_series_dfs:\n",
        "        ordered_dfs = []\n",
        "        # Use the order from params_map keys for consistent output order\n",
        "        for code in params_map.keys():\n",
        "            if code in collected_series_dfs:\n",
        "                ordered_dfs.append(collected_series_dfs[code])\n",
        "\n",
        "        if ordered_dfs:\n",
        "            combined = pd.concat(ordered_dfs, axis=1)\n",
        "            return combined\n",
        "        else:\n",
        "            return pd.DataFrame() # In case none of the desired codes were found\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Get data\n",
        "df = get_usgs_data(site, param_code)\n",
        "\n",
        "if df.empty:\n",
        "    # print(\"No data available for selected parameters.\") # Commenting out display for script\n",
        "    pass\n",
        "else:\n",
        "    df.index.name = 'date'\n",
        "    # print(\"\\nRecycled water parameters:\") # Commenting out display for script\n",
        "    # print(df.head()) # Commenting out display for script\n",
        "\n",
        "\n",
        "\n",
        "# print(\"Missing values in Recycled_water_data:\") # Commenting out display for script\n",
        "# display(df.isnull().sum()) # Commenting out display for script\n",
        "\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "# print(\"Missing values in Recycled_water_data:\") # Commenting out display for script\n",
        "# display(df.isnull().sum()) # Commenting out display for script\n",
        "\n",
        "\n",
        "# Dyeing Process dataset\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Example: load CSV (replace url or file path accordingly)\n",
        "url = 'https://raw.githubusercontent.com/NidaSaleem1/ARN/refs/heads/main/Textile_data.csv'\n",
        "response = requests.get(url)          # API-style HTTP GET request\n",
        "csv_data = response.text              # raw CSV text returned by the server\n",
        "\n",
        "textile_data = pd.read_csv(StringIO(csv_data))  # convert CSV text → DataFrame\n",
        "\n",
        "line1_columns = [col for col in textile_data.columns if 'date' in col or 'dyeing_line1' in col]\n",
        "dyeing_line1_data = textile_data[line1_columns]\n",
        "\n",
        "# display(dyeing_line1_data.head()) # Commenting out display for script\n",
        "\n",
        "\n",
        "# print(\"Missing values in dyeing_line1_data:\") # Commenting out display for script\n",
        "# display(dyeing_line1_data.isnull().sum()) # Commenting out display for script\n",
        "\n"
      ],
      "metadata": {
        "id": "fiwslg44f8Yw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dyeing data is federated because PolyTex Enterprise has multiple dyeing plants/units, each plants keep its own database (not merged into one main database)\n",
        "# Data is later accessed through a shared reporting interface\n",
        "# Dyeing data is collected and controlled via sensors,transmitters, and process control systems, so it belongs to level 1(Sensing and Process Control.) on S95 model of PolyTex\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define a simple Plant class if it's not defined elsewhere\n",
        "# This is a placeholder; a more complete Plant class might be defined in a separate cell or file.\n",
        "class Plant:\n",
        "    def __init__(self, id=None, name=None, city=None, type=None):\n",
        "        self.id = id\n",
        "        self.name = name\n",
        "        self.city = city\n",
        "        self.type = type\n",
        "        self.renewable_energy_data = None # Placeholder for data storage\n",
        "\n",
        "# Assume 'supabase' client is available if needed, otherwise this will cause an error.\n",
        "supabase = None # Placeholder; replace with actual Supabase client initialization if applicable\n",
        "\n",
        "def generate_renewable_energy_data(self):\n",
        "    \"\"\"\n",
        "    Fetch renewable energy forecast data from Electricity Maps API based on plant location.\n",
        "    Returns forecast data including renewable percentage, carbon intensity, and electricity mix.\n",
        "    \"\"\"\n",
        "    # Get coordinates for the plant's city\n",
        "    city_coords = {\n",
        "        \"Toronto\": (43.6532, -79.3832),\n",
        "        \"Houston\": (29.7604, -95.3698),\n",
        "        \"Vancouver\": (49.2827, -123.1207)\n",
        "    }\n",
        "\n",
        "    # Map cities to Electricity Maps zones\n",
        "    # You can find more wherezones at: https://portal.electricitymaps.com/developer-hub/api/getting-started#geographical-coverage\n",
        "    city_zones = {\n",
        "        \"Toronto\": \"CA-ON\",  # Ontario, Canada\n",
        "        \"Houston\": \"US-TEX-ERCO\",  # Texas ERCOT\n",
        "        \"Vancouver\": \"CA-BC\"  # British Columbia, Canada\n",
        "    }\n",
        "\n",
        "    # Get zone for this plant's city\n",
        "    if self.city in city_zones:\n",
        "        zone = city_zones[self.city]\n",
        "        if self.city in city_coords:\n",
        "            latitude, longitude = city_coords[self.city]\n",
        "        else:\n",
        "            latitude, longitude = 43.6532, -79.3832\n",
        "    else:\n",
        "        # Default to Toronto/Ontario if city not found\n",
        "        zone = \"CA-ON\"\n",
        "        latitude, longitude = 43.6532, -79.3832\n",
        "        # print(f\"Warning: City '{self.city}' not in known zones, using CA-ON\") # Commenting out display for script\n",
        "\n",
        "    # Electricity Maps API endpoint for renewable energy forecast\n",
        "    # Documentation: https://portal.electricitymaps.com/developer-hub/api/reference/renewable-energy\n",
        "    url = \"https://api.electricitymaps.com/v3/carbon-intensity/past?datetime=2025-11-09+01%3A05\"\n",
        "\n",
        "    headers = {\n",
        "        \"auth-token\": \"r1aC7eCYktQufOGUxb3e\"\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        \"zone\": zone,\n",
        "        \"temporalGranularity\": \"hourly\"  # Options: '5_minutes', '15_minutes', 'hourly'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # Extract forecast data\n",
        "        if data and len(data) > 0:\n",
        "            # Convert to DataFrame\n",
        "            forecast_list = []\n",
        "            for entry in data:\n",
        "                forecast_list.append({\n",
        "                    \"timestamp\": pd.to_datetime(entry.get(\"datetime\")),\n",
        "                    \"renewable_percentage\": entry.get(\"renewablePercentage\", 0),\n",
        "                    \"carbon_intensity\": entry.get(\"carbonIntensity\", 0),\n",
        "                    \"is_estimated\": entry.get(\"isEstimated\", False),\n",
        "                    \"estimation_method\": entry.get(\"estimationMethod\", \"\")\n",
        "                })\n",
        "\n",
        "            renewable_df = pd.DataFrame(forecast_list)\n",
        "            renewable_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "            # Add metadata\n",
        "            renewable_df['zone'] = zone\n",
        "            renewable_df['latitude'] = latitude\n",
        "            renewable_df['longitude'] = longitude\n",
        "\n",
        "            self.renewable_energy_data = renewable_df\n",
        "\n",
        "            # Store in Supabase\n",
        "            if supabase is not None:\n",
        "                self.store_renewable_energy_to_supabase(renewable_df)\n",
        "\n",
        "            # print(f\"Renewable energy forecast data fetched successfully for {self.name} in {self.city} (Zone: {zone})\") # Commenting out display for script\n",
        "            # print(f\"Forecast contains {len(renewable_df)} hours of data\") # Commenting out display for script\n",
        "            return renewable_df\n",
        "        else:\n",
        "            # print(f\"Warning: No forecast data found in API response\") # Commenting out display for script\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # print(f\"Error fetching renewable energy data: {e}\") # Commenting out display for script\n",
        "        if hasattr(e, 'response') and e.response is not None:\n",
        "            # print(f\"Response status: {e.response.status_code}\") # Commenting out display for script\n",
        "            # print(f\"Response body: {e.response.text}\") # Commenting out display for script\n",
        "            pass\n",
        "        return None\n",
        "    except KeyError as e:\n",
        "        # print(f\"Error parsing renewable energy data: {e}\") # Commenting out display for script\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        # print(f\"Unexpected error: {e}\") # Commenting out display for script\n",
        "        return None\n",
        "\n",
        "# Attach method to Plant base class (works for all plant types)\n",
        "Plant.generate_renewable_energy_data = generate_renewable_energy_data\n",
        "# df.head() # Commenting out display for script\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1AaP2Olvf3C_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing Renewable Energy Data to Supabase - Miguel\n",
        "def store_renewable_energy_to_supabase(self, renewable_df):\n",
        "    \"\"\"\n",
        "    Store renewable energy forecast data to Supabase database.\n",
        "    This allows for historical tracking and energy optimization decisions.\n",
        "    \"\"\"\n",
        "    if supabase is None:\n",
        "        # print(\"Supabase client not initialized. Skipping database storage.\") # Commenting out display for script\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Convert DataFrame to dictionary for Supabase\n",
        "        df_copy = renewable_df.reset_index()\n",
        "\n",
        "        # Prepare data for insertion\n",
        "        for _, row in df_copy.iterrows():\n",
        "            record = {\n",
        "                \"plant_id\": self.id,\n",
        "                \"plant_name\": self.name,\n",
        "                \"plant_city\": self.city,\n",
        "                \"plant_type\": self.type,\n",
        "                \"timestamp\": row['timestamp'].isoformat() if pd.notna(row['timestamp']) else None,\n",
        "                \"renewable_percentage\": float(row['renewable_percentage']) if pd.notna(row['renewable_percentage']) else None,\n",
        "                \"carbon_intensity\": float(row['carbon_intensity']) if pd.notna(row['carbon_intensity']) else None,\n",
        "                \"zone\": str(row['zone']) if pd.notna(row['zone']) else None,\n",
        "                \"latitude\": float(row['latitude']) if pd.notna(row['latitude']) else None,\n",
        "                \"longitude\": float(row['longitude']) if pd.notna(row['longitude']) else None,\n",
        "                \"is_estimated\": bool(row['is_estimated']) if pd.notna(row['is_estimated']) else None,\n",
        "                \"estimation_method\": str(row['estimation_method']) if pd.notna(row['estimation_method']) else None\n",
        "            }\n",
        "\n",
        "            # Insert into Supabase (assuming table name is 'renewable_energy')\n",
        "            response = supabase.table(\"renewable_energy\").insert(record).execute()\n",
        "\n",
        "        # print(f\"Renewable energy data stored in Supabase for {self.name} ({len(df_copy)} records)\") # Commenting out display for script\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error storing data to Supabase: {e}\") # Commenting out display for script\n",
        "        pass\n",
        "\n",
        "# Attach method to Plant base class\n",
        "Plant.store_renewable_energy_to_supabase = store_renewable_energy_to_supabase\n",
        "# df.head() # Commenting out display for script"
      ],
      "metadata": {
        "id": "gSRysjDxfxGu"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}